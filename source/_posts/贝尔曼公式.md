---
title: 贝尔曼公式
date: 2025-11-26 17:04:01
tags:
- 强化学习
- 控制理论
categories:
- 控制
cover: https://cdn.jsdelivr.net/gh/xiaoyuewhut/image@main/d7e44f1147cc52adb7b175c0ece4ab39.png
---


# 考虑一个简单的例子，四个格子，从不同的起点出发：
![](https://cdn.jsdelivr.net/gh/xiaoyuewhut/image@main/image-20250427225353105.png)
一直转圈圈循环，比如从$s_1$出发，它的*return*：
$$
\begin{aligned}
v_1 &= r_1+\gamma r_2+\gamma^2 r_3+\cdots\\
&=r_1 + \gamma (r_2 + r_3 + \cdots)\\
&=r_1 + \gamma v_2
\end{aligned}
$$
$v_i$代表从$s_i$出发，一直循环下去的*return*。
类似的：
$$
\begin{aligned}
v_2 &= r_2 + \gamma v_3\\
v_3 &= r_3 + \gamma v_4 \\
v_4 &= r_4 + \gamma v_1
\end{aligned}
$$
（左脚踩右脚可以上天了）

# 如何解这个方程，求出$v_i$？
$$
\underbrace{
\begin{bmatrix}
v_1 \\
v_2 \\
v_3 \\
v_4
\end{bmatrix}
}_{\mathbf{v}} = 
\begin{bmatrix}
r_1\\
r_2\\
r_3\\
r_4
\end{bmatrix} + 
\begin{bmatrix}
\gamma v_2\\
\gamma v_3\\
\gamma v_4\\
\gamma v_1
\end{bmatrix} = 
\underbrace{
\begin{bmatrix}
r_1\\
r_2\\
r_3\\
r_4
\end{bmatrix}
}_{\mathbf{r}}+ \gamma
\underbrace{
\begin{bmatrix}
0 & 1 & 0 & 0\\
0 & 0 & 0 & 0\\
0 & 0 & 0 & 1\\
1 & 0 & 0 & 0
\end{bmatrix}}_{\mathbf{P}}
\underbrace{
\begin{bmatrix}
v_1\\
v_2\\
v_3\\
v_4
\end{bmatrix}}_{\mathbf{v}}
$$
which can be written as
$$
\mathbf{v}=\mathbf{r}+\gamma \mathbf{Pv}
$$
这就是**贝尔曼公式**（**Bellman equation**）

# State value
考虑以下这个单步过程：
$$
S_t \xrightarrow{A_t}R_{t+1},S_{t+1}
$$

拓展到多步过程，得到一个multi-step trajectory：
$$
S_t \xrightarrow{A_t}R_{t+1},S_{t+1} \xrightarrow{A_{t+1}}R_{t+2},S_{t+2} \xrightarrow{A_{t+2}}R_{t+3},S_{t+3} \xrightarrow{A_{t+3}} \cdots
$$

The discounted return is
$$
G_t=R_{t+1}+\gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots
$$

实际上*state value*就是$G_t$的expectation：
$$
v_\pi(s)=\mathbb{E}[G_t|S_t=s]
$$
所以这是一个关于初始状态$s$的函数，也是一个关于策略（policy）$\pi$的函数。

# State value和前面的Return有什么区别？
> + *Return*针对的是单个*trajectory*
> + *State value*是多个*trajectory*的期望

# 如何推导 Bellman equation ？
把前面的式子推导：
$$
G_t = R_{t+1}+ \gamma G_{t+1}
$$
代入：
$$
\begin{aligned}
v_\pi(s) &= \mathbb{E}[G_t|S_t=s]\\
&=\mathbb{E}[R_{t+1}+\gamma G_{t+1} | S_t=s]\\
&=\mathbb{E}[R_{t+1} | S_t=s] + \gamma \mathbb{E}[G_{t+1} | S_t=s]
\end{aligned}
$$

这样就得到了两个期望。
$$
\begin{aligned}
\mathbb{E}[R_{t+1} | S_t=s]&=\sum_a \pi(a|s) \mathbb{E}[R_{t+1}|S_t=s,A_t=a]\\
&=\sum_a \pi(a|s) \sum_r p(r|s,a)r
\end{aligned}
$$

$$
\begin{aligned}
\mathbb{E}[G_{t+1} | S_t=s] &= \sum_{s'}v_{\pi}(s') \sum_ap(s'|s,a)\pi(a|s)
\end{aligned}
$$

两个相加
$$
\begin{aligned}
v_\pi(s)&=\mathbb{E}[R_{t+1} | S_t=s]+\mathbb{E}[G_{t+1} | S_t=s]\\
&=
\underbrace{
\sum_a \pi(a|s) \sum_r p(r|s,a)r}_\text{mean of immediate rewards}
+ 
\underbrace{
\gamma \sum_a \pi(a|s) \sum_{s'} p(s'|s,a)v_\pi(s')
}_\text{mean of future rewards}\\
&=
\sum_a \pi(a|s)[\sum_r p(r|s,a)r + \gamma \sum_{s'} p(s'|s,a)v_\pi(s')]
\end{aligned}
$$
这就是贝尔曼公式，描述了不同状态的*state value*之间的关系。

+ $v_\pi(s)$ 和 $v_\pi(s')$ 是待求的 *state value*
+ $\pi(a,s)$ 是给定的 *policy*，实际上解这个方程的过程就是在评估 *policy* 的好坏，叫做 “*policy evaluation*”
+ $p(r|s,a)r$ 和 $p(s'|s,a)r $叫做“*dynamic model*”。

# 贝尔曼公式的矩阵和向量形式（matrix-vector form）
考虑之前推导的*bellman equation*：
$$
\begin{aligned}
v_\pi(s)&=\sum_a \pi(a|s)[\sum_r p(r|s,a)r + \gamma \sum_{s'} p(s'|s,a)v_\pi(s')]
\end{aligned}
$$
这个式子中包含两个*state value*：$v_\pi(s)$和$v_\pi(s')$，光凭这一个式子求*state value*是不可能的。

如果有*n*个*state*，那么就会有*n*个这样的*equation*，联立到一起，就可以得到一组**线性方程组**，进而可以写成**矩阵向量形式**。

Rewrite the Bellman equation as
$$
v_\pi(s)=r_\pi(s)+\gamma \sum_{s'}p_\pi(s'|s)v_\pi(s')
$$
where
$$
r_\pi(s) \triangleq \sum_a \pi(a|s)\sum_r p(r|s,a)r
$$
这个就表示从当前状态$s$出发，得到的*immediate rewards*的平均值，很好理解。
$$
p_\pi(s'|s) \triangleq \sum_a \pi(a|s) p(s'|s,a)r
$$
这个代表从状态$s$到$s'$的概率。

**给所有状态加上index**：
$$
v_\pi(s_i)=r_\pi(s_i)+\gamma \sum_{s_j}p_\pi(s_j|s_i)v_\pi(s_j)
$$

所有式子合到一起
$$
v_\pi=r_\pi+\gamma P_\pi v_\pi
$$

+ $v_\pi=[v_\pi(s_1), v_\pi(s_2), \cdots, v_\pi(s_n)]^T \in \mathbb{R}^n$
+ $r_\pi=[r_\pi(s_1), r_\pi(s_2), \cdots, r_\pi(s_n)]^T \in \mathbb{R}^n$
+ $P_\pi \in \mathbb{R}^{n \times n}$，where $[P_\pi]_{ij} = p_\pi(s_j|s_i)$，是*state transition matrix*。

![](https://cdn.jsdelivr.net/gh/xiaoyuewhut/image@main/image-20250428095036687.png)

# 求解矩阵向量形式的 bellman equation
+ 一种是通过逆矩阵求解
$$
v_\pi=(I-\gamma P_\pi)^{-1}r_\pi
$$

**注：** **在实际中并不会采用这样的方法，因为当state维度很高时，求逆矩阵所需计算量也会很大**。

+ 另一种是迭代方法（*iterative solution*）
$$
v_{k+1}=r_\pi+\gamma P_\pi v_{k}
$$

首先有一个猜想值$v_0$，根据式子，可以一直往下求$v_1,v_2,\cdots$，一直求下去，可以知道当$k \rightarrow \infty$时，$v_k$一定是收敛的。

# Action value

## 和state value有什么区别？
+ state value指的是agent从一个stata出发，得到的所有average return;
+ action value指的是agent从一个state出发，并且采取一个action，所得到的average return。

## 定义
$$
q_\pi(s,a)=\mathbb{E}[G_t|S_t=s,A_t=a]
$$
+ 所以$q_\pi(s,a)$是一个pair的形式
+ $q_\pi(s,a)$depends on the policy $\pi$

建立起state value和action value的联系
$$
v_\pi(s) = \sum_a \pi(a|s) q_\pi(s,a)
$$

$$
\rightarrow \text{state value} = \sum \text{policy} \times \text{action value}
$$

看式$\eqref{eq:statevalue}$，可以发现方括号内就是一个action value $q_\pi(s,a)$：
$$
q_\pi(s,a)=\sum_r p(r|s,a)r + \gamma \sum_{s'} p(s'|s,a)v_\pi(s')
$$
$$
\rightarrow \text{action value} = \sum \text{immediate reward} \times \gamma \cdot \sum \text{state value}
$$

这是一个由state value推action value的过程。

# 贝尔曼最优公式

## 如果当前policy不太好，怎么去改善它？
> 通过**action value**

比如在$s_1$状态，评估所有action的action value $q_\pi(s_1,a_j)$，选最大的那个
$$
a^*=\text{arg max}_aq_\pi(s_1,a)=a_?
$$

**只要对每个state都选择action value最大的那个action，如此迭代下去，一定会得到一个optimal policy，如何从数学上证明它，就省略了吧。。。。。。。**

## 什么是最优（optimal）？

实际上就是这个policy，对**所有state**的state value都优于**其他任意policy**。

但这是理想情况，可能存在以下问题：

+ Does the optimal policy exist? 可能这个policy在某些state达到最优，但某些state并不能达到最优？
+ Is the optimal policy unique? 是否存在多个最优策略？
+ Is the optimal policy stochastic or deterministic?
+ How to obtain the optimal policy?

## 求解贝尔曼最优公式
$$
v = \mathop{\text{max}}_\pi(r_\pi+\gamma P_\pi v)
$$

## optimal policy是否存在且唯一？
根据压缩映射理论(**contraction mapping theorem**)，存在最优state value $v^*$，且唯一。

固定了$v^*$，就可以假设
$$
\begin{aligned}
\pi^* &=\text{arg max}(r_\pi+ \gamma P_\pi v^*) \\
v^* &=r_{\pi^*}+\gamma P_{\pi^*}v^*
\end{aligned}
$$
也可以证明policy的optimality，但optimal policy不一定唯一。

## 哪些因素决定了最优策略？

+ Rward design: $r$
+ System model: $p(s'|s, a)$, $p(r|s,a)$
+ Discount rate: $\gamma$
