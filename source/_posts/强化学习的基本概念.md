---
title: 强化学习的基本概念
date: 2025-11-26 16:47:03
tags:
- 强化学习
- 控制理论
categories:
- 控制
cover: https://cdn.jsdelivr.net/gh/xiaoyuewhut/image@main/0ad6b021683e7f634440424193025e54.png
---
# State
*state*是被控对象的状态，对应车辆的$x,y,\theta,v,a$等。
$$
s=\begin{bmatrix} x \\
y\\
\theta \\
\vdots
\end{bmatrix}
$$
状态空间（*State Space*）$\mathcal{S} = \{ s_{i} \}_{i=1,2,\cdots}$，代表一个集合。
# State transition
当采取一个*action* $a_{i+1}$，*agent*的状态会从一个$s_i$变成另一个$s_{i+1}$，这个过程就叫做*state transition*，用以下这个式子表示：
$$
s_i \xrightarrow{a_{i+1}} s_{i+1}
$$
![](https://cdn.jsdelivr.net/gh/xiaoyuewhut/image@main/image-20250427213234863.png)
然而这种表格表达的*state transition*只能表达有限个确定性（deterministic）的例子。

更多是用*state transition probability*表达，比如说在$s_1$时刻，如果做出*action* $a_2$，那么一定会达到$s_2$状态，用概率公式表达就是：
$$
p(s_2|s_1,a_2)=1
$$

# Policy
*polity*告诉*agent*在某个*state*应该采取什么*action*，用$\pi$表示，比如：
$$
\pi(a_1,s_1)=0
$$
这个就表示在$s_1$时，采取$a_1$的概率为0。

# Reward
在采取一个*action*之后，会得到一个数*reward*。

+ 如果*reward*为正，代表这样的*action*是被鼓励的（*encouragement*）；
+ 如果*reward*为负，代表这样的*action*是要被惩罚的（*punishment*）。

还是可以用*probability*来表达：
$$
p(r=-1|s_1,a_1)=1
$$
值得注意的是，当前*reward*只取决于当前*state*和*action*，而不是下一个*state*。

# Trajectory
实际上就是*state transition*的链式表达。

# Return
针对一个*trajectory*，它的*return*是这个过程中的*reward*累加，所以说不同的*policy*会得到不同的*trajectory*，也就会得到不同的*return*。拥有更大的*reward*的*policy*当然是更好的。

# Discounted return
一个*trajectory*可能是无限的，比如到了终点，任务已经完成了，但是*reward*还在累加，导致奖励发散。

这个时候就需要引入一个*discounted return* $\gamma$
$$
\gamma \in [0, 1)
$$

$$
\begin{aligned}
\text{discounted return} &= 0 + \gamma r_1 + \gamma^2 r_2 + \gamma^3 r_3 + \cdots\\
\end{aligned}
$$
因此，一般这个*return*也就保证了有界收敛。

# Episode
一个*episode*通常是一个有界的*trajectory*（*episode*也称*trial*），被称为*episodic tasks*。

有些任务是没有terminal states的，意味着这个*trajectory*会永远继续下去，被称为*continuing tasks*。

# MDP（Markov decision process）
马尔科夫决策过程
## MDP的要素
+ *Sets*：*State*，_Action_，和*Reward*各自的集合。
+ *Probability Distribution*：*State transition probability*，*Reward probability*。
+ *Policy*
+ _Markov property_：不考虑历史，只考虑当前状态和动作。

