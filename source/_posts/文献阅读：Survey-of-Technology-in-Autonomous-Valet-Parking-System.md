---
title: 文献阅读：Survey of Technology in Autonomous Valet Parking System
date: 2025-12-03 11:15:01
tags:
- 代客泊车
- 智能驾驶
- 泊车
categories:
- 文献阅读
cover: 
---
# 引言
>Jo Y, Ha J, Hwang S. Survey of technology in autonomous valet parking system[J]. International Journal of Automotive Technology, 2023, 24(6): 1577-1587.

代客泊车（Valet parking）是法语“valet”（意为代泊）和英语“parking”组成的复合词，指的是由管理人员代替车主泊车的服务。自动代客泊车（AVP）系统将代客泊车与自动驾驶车辆相结合，这项技术能**让车辆自动行驶到停车位完成泊车，并且在泊车后用户召唤时将车辆移动到指定位置**。通常情况下，该系统无需人工操作就能在行驶和泊车场景中对车辆进行控制。

一般AVP的流程如下：
![](https://cdn.jsdelivr.net/gh/xiaoyuewhut/image@main/20251203112622015.png)

其中搜索路径需要考虑很多约束：车辆的转弯半径、转向速度、障碍物、**停车位类型**。

Return Driving Process 就是将车辆移动到用户可以再次上车的地方。通过追踪先前流程中的路径来实现。

# 搜索过程

搜索路径的基础是车辆自身以及周围物体的**定位**，比如：
+ 使用GPS+IMU来确定车辆的速度和姿态
+ 使用摄像头用于定位（单目、立体、鱼眼）
+ 使用雷达传感器（短程、远程）

通常，这些传感器用于SLAM中，SLAM能够同时创建地图并估计车辆的pose，但是SLAM也有两种算法：
+ **直接法**
利用原始数据，直接跟踪传感器移动时的变化的数据强度来估计位姿。但是容易受到光照强度变化的影响，虽然可能精度会比较高，但是无法实现重定位——即根据已创建的地图来估计当前位置。这是因为旧地图数据和新输入数据之间的匹配精度较低。
+ **基于特征的方法**
大多采用这种方法。首先必须从传感器信息中获取周围物体的特征点。将来自同一物体的特征点投影到两个不同的传感器坐标系后，通过计算投影点的几何关系可以估计物体的位置。这可以利用对极几何来实现，并且通过这种关系可以得到两个传感器之间的位置差异。对极几何是投影到传感器坐标系中的点的几何。这些投影点是由三维空间中的一个点生成的。
![](https://cdn.jsdelivr.net/gh/xiaoyuewhut/image@main/20251203115357379.png)

点 $X$ 在图像A中投影到$x_1$上，在B中投影到$x_2$，点$c_1$和$c_2$被称为对极点 epipoles，就是连接两个传感器原点的直线与坐标平面的交点。直线$L_1$和$L_2$被称为极线，就是连接投影点和对极点。

**对于图像A中固定的点$x_1$，无论三维点 $X$ 在空间中沿射线（从$O$出发经过$x_1$的直线）如何远近移动（即深度变化），$x_1$在图像A中的位置始终不变。**

但与此同时：
+ 随着 $X$ 的深度变化，它在图像B中的投影点 $x_2$ 会沿着对极线 $L_2$ 滑动
+ 所以 $x_2$ 的位置 不是唯一的——它取决于 $X$ 的实际深度

> **所以这种对极几何关系本质上是一个约束，它极大地缩小了匹配点的搜索范围**：如果没有对极几何约束：要在图像B中找到 $x_1$ 的对应点 $x_2$，需要搜索整个图像。但是现在只需要沿着直线搜索就行。

因此引入了本质矩阵$E$，还有基础矩阵$F$：
+ 本质矩阵：描述的是规范化坐标系下的几何关系
+ 基础矩阵：加入了摄像头内参，描述像素坐标系下的关系

接下来讲讲**视差（Disparity）**：

视差是指同一个三维点在两个相机图像中投影位置的像素坐标差。简单说，看同一个物体，左右眼看到的图像位置不一样，这个差异就是视差。
![](https://cdn.jsdelivr.net/gh/xiaoyuewhut/image@main/20251203124213841.png)
$$
\text{Disparity} = |x_1 - x_2| = \frac{\text{Baseline} \cdot f}{Z}
$$
其中，$\text{Baseline}$是两个相机中心 O 与 O' 之间的距离（单位：米），$f$是相机的焦距（单位：像素），$Z$是深度，也就是物体到相机的实际距离（单位：米）

> 关键理解：为什么视差能反映深度？
**视差与深度成反比**。 这非常符合直觉

想象一下：把手指放在眼前10 cm处，交替用左右眼看。左眼看到手指在右边，右眼看到手指在左边，位置差异巨大，公式体现：$Z$ 很小 → $\text{Baseline} \cdot f/Z$ 的值很大 → 视差大

但把手指放在10米外，交替用左右眼看左右眼看到的位置几乎相同，差异微乎其微。

因此，在AVP中，车辆通常配备双目相机（两个水平放置的摄像头）或相机+激光雷达组合：
1. 特征匹配：在两帧图像中找到同一个角点、边缘等特征点
2. 计算视差：测量该特征在两图中的像素坐标差
3. 深度计算：代入公式，得到该点的实际距离
4. 建图：将大量特征点的深度信息融合，重建停车场三维地图

# 停车位检测
![](https://cdn.jsdelivr.net/gh/xiaoyuewhut/image@main/20251203125632326.png)
停车位在形状和大小上各不相同，取决于其用途（purpose）。因此，仅使用计算机视觉技术进行停车位识别时，需要**为每种类型实现不同的技术**。

+ 传统计算机视觉方法（如线检测、特征点检测）需要针对不同类型调整
+ 深度学习方法（如CNN、R-CNN）通过大规模数据学习来泛化识别能力
+ 混合方法：先用计算机视觉快速定位候选区域，再用深度学习精细分类

> 要注意的一个东西：**停车位类型主要在"识别检测阶段"考虑，而在"路径规划阶段"（如A*算法）主要依赖的是几何信息而非语义类型标签。**

# 泊车路径生成
主要介绍了自主泊车路径生成的两种技术路线
1. **基于算法的方法（传统主流）**
根据车位几何形状和车辆当前位姿，通过数学公式直接计算可行路径，主要包括：
+ **阿克曼转向模型**：考虑车辆最小转弯半径约束
+ **最优控制问题**：将泊车建模为带约束的优化问题
+ **基于网格的路径规划**：如改进的A算法（Hybrid A）
+ **快速探索随机树（RRT）**：适用于狭窄空间的全局路径搜索

**特点**：解析性强、可解释性高，但对复杂边界条件处理能力有限。

2. **强化学习**
没什么好说的，奖励条件就是**泊车精度指标**，包括：
+ 四角到车位线的距离
+ 到目标位置的距离
+ 最终转向角
![](https://cdn.jsdelivr.net/gh/xiaoyuewhut/image@main/20251203130312238.png)

# 总结
没什么值得看的，非常短的综述，韩国人瞎糊弄。

